I am the colleague of Jon who complained about array index notation. Let me
clarify the issue a little bit.

Many of Jon's examples in Python confuses (not his fault but Python's ) *table*
with *multi-dimenstional array*. This can be easily seen as both selection (filter)
and projection (getting the ```x``` component) are done as array indices. There are
many problems with this notation. The first one is that array index operators
are not easily composable thus does not encourage decomposition of query and
incremental building up of complex queries. This greatly diminishes users'
ability to do exploratory data analysis. To see why, let's take Jon's example
and try to extend his query:

```python
I = numpy.where(table['id'] > 16000000)
xi = table['x'][I]
```

Now what happens when I find out that I would like to further restrict the range
of ```id```, since there are too many of them from the previous result? One way
of doing it is to have compound Boolean expression inside the ```numpy.where()```.

```python
I = numpy.where((table['id'] > 16000000) & (table['id'] < 16005000))
xi = table['x'][I]
```

That looks O.K. How about further restricting the result to entries with
positive ```y``` values?

```python
I = numpy.where((table['id'] > 16000000) & (table['id'] < 16005000) & (table['y'] > 0))
xi = table['x'][I]
```

As you can see, the size and complexity of the compound Boolean expression will
quickly grow out of control. People will have hard time parsing and editing the
expression. What is worse, there is probably no way to truly *linearize* the
expression and thus decompose the query:

```python
I1 = numpy.where(table['id'] > 16000000)
I2 = numpy.where(table['id'] < 16005000)
xi = table['x'][I1][][I2]
```

Since the table returned by ```table['x'][I1]``` has different number of entries
than the original ```table```, ```I2``` as calculated in the second line would
not be suitable for the third line. The only way of doing it would be:

```python
I1 = numpy.where(table['id'] > 16000000)
x1 = table[][I1]
I2 = numpy.where(x1['id'] < 16005000)
x2 = x1['x'][I2]
```

Again, users are forced to create a whole lot of temporary variables. By the way,
I am also not so sure if ```table[][I1]['x'][I2]``` is syntactically correct for
the reason that subsetting/indexing a *table* may or may not return a *table* and
we may have to put a lot of parenthesis as well ```(table[][I1])['x'][I2]```.

The other problems is that array index notation also exposes the column major vs.
row major point of view of the underlying implementation. As you can see in his
examples, one of them uses row major point of view while the other uses column
major:

```python
# row major
xi = table['x'][I]
# column major
xi = D.loc[D.id > 16000000, ['x']]
```

How and why programmers have to remember which one is which?

All of these problems are beautifully solved by something like R/dplyr or Haskell
which provides the bind/pipe operator. For example, I can easily linearize and
decompose my query into little pieces:

```R
table %>% filter(id > 16000000) %>% filter(id < 16005000) %>% select(x)
```

The major reason this notation works is that at each stage of the *pipeline*,
*table* are used as both the input type and output type thus having *algebraic
closure*. We can in theory chain as many stages as we want with worry about if
we can not keep going because some operations are not supported thus falling of
the cliff all the sudden. There is also no need to worry about if the table is
actually row or column major. The concept is never in the play in the first
place.
