After creating some data backed in a memmapped numpy array, I did a simple
filter and selection. There are over 16 million rows in this table.

*numpy* does it in 0.03s, where *table* is a dict of numpy arrays:
```python
I = numpy.where(table['id'] > 16000000)
xi = table['x'][I]
```
### Updated Dec 16 ###

*D* is a *pandas* data frame created directly from the dictionary. 
<del>It's just as fast as *numpy*. Just make sure you use the 
right indexing, such as .loc, .iloc, and .ix. [] can be very slow.</del>
*Actually, doing some additional testing, I am finding that* pandas
*is an order of magnitude slower, what I was seeing before. I'm not
sure what is going on here, but the operation below is .3s (I may
have read it incorrectly the first time.)*
```python
xi = D.loc[D.id > 16000000, ['x']]
```
<del>Though, we can see we are doing it out-of-core with the same memmapped 
*numpy* array with the nice *pandas* DSL. Yay!</del>
What I need to test next is
if I can get pandas to build out-of-core indices. *And, I need to verify
that I am still seeing that* pandas *is an order of magnitude slower
than numpy, which was my original conclusion prior to posting this.*

And, here the loser is the iterator/generator method. sx and si are
"streaming" versions, created by numpy.nditer. PyObject overhead
rears its ugly head and this takes 47s... So, much for that.
```python
f = filter(lambda i: i[1] > 16000000, zip(sx, si))
xi = [i[0] for i in f]
```
