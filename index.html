<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="chrome=1"><title>Technical Blag by LANL Data Science At Scale</title><link rel="stylesheet" href="stylesheets/styles.css"><link rel="stylesheet" href="stylesheets/pygment_trac.css"><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"><!--if lt IE 9script(src='//html5shiv.googlecode.com/svn/trunk/html5.js')--></head><body><div class="wrapper"><header><h1>Los Alamos National Lab Data Science at Scale Team</h1><p class="view"><a href="http://datascience.lanl.gov">Data Science at Scale Website </a></p><p class="view"><a href="https://github.com/DataScienceAtScale">The Project on GitHub </a></p><p class="view"><small><h3 id="welcome-to-the-data-science-at-scale-blog">Welcome to the Data Science at Scale Blog</h3>
<p>Extremely large datasets and extremely high-rate data streams are becoming increasingly common due to the operation of Moore&apos;s Law as applied to sensors, embedded computing, and traditional high-performance computing. Interactive analysis of these datasets is widely recognized as a new frontier at the interface of information science, mathematics, computer science, and computer engineering. Text searching on the web is an obvious example of a large dataset analysis problem; however, scientific and national security applications require far more sophisticated interactions with data than text searches. These applications represent the &apos;data to knowledge&apos; challenge posed by extreme-scale datasets in, for example, astrophysics, biology, climate modeling, cyber security, earth sciences, energy security, materials science, nuclear and particle physics, smart networks, and situational awareness. In order to contribute effectively to Los Alamos National Lab&apos;s overall national security mission, we need a strong capability in Data Science at Scale. This capability rests on robust and integrated efforts in data management and infrastructure, visualization and analysis, high-performance computational statistics, machine learning, uncertainty quantification, and information exploitation. The Data Science at Scale capability provides tools capable of making quantifiably accurate predictions for complex problems with the efficient use and collection of data and computing resources.</p>
<p>LA-UR-15-20107</p>
</small></p><p class="view"><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p></header><section><h3>SIAM CSE 2015 in March</h3><a name="SIAM-CSE-2015-in-March" href="#SIAM-CSE-2015-in-March" class="anchor"><span class="octicon octicon-link"></span></a><small>March 6, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>I&#39;ll be giving another 
(Python tutorial)[<a href="http://www.siam.org/meetings/cse15/mini.php">http://www.siam.org/meetings/cse15/mini.php</a>] 
at SIAM CSE 2015. This will be on Sunday, March 15, 2015.</p>
<p>You can find the materials (here)
[<a href="https://github.com/JosephCottam/PythonSIAM2015">https://github.com/JosephCottam/PythonSIAM2015</a>].</p>
<p>Also, I will also be representing LANL at the SIAM Career Fair on 
Saturday, March 14, 2015. Come by and say &quot;hi!&quot; and drop off your CV.</p>
<p>See you in Utah!</p>
</p><a href="#SIAM-CSE-2015-in-March"><small>Link to this post</small></a><hr><br><h3>Notation, notation, notation</h3><a name="Notation,-notation,-notation" href="#Notation,-notation,-notation" class="anchor"><span class="octicon octicon-link"></span></a><small>January 7, 2015 -- <a href="https://github.com/ollielo">Li-Ta Lo</a><br></small><p><p>I am the colleague of Jon who complained about array index notation. Let me
clarify the issue a little bit.</p>
<p>Many of Jon&#39;s examples in Python confuses (not his fault but Python&#39;s ) <em>table</em>
with <em>multi-dimenstional array</em>. This can be easily seen as both selection (filter)
and projection (getting the <code>x</code> component) are done as array indices. There are
many problems with this notation. The first one is that array index operators
are not easily composable thus does not encourage decomposition of query and
incremental building up of complex queries. This greatly diminishes users&#39;
ability to do exploratory data analysis. To see why, let&#39;s take Jon&#39;s example
and try to extend his query:</p>
<pre><code class="lang-python">I = numpy.where(table[&#39;id&#39;] &gt; 16000000)
xi = table[&#39;x&#39;][I]
</code></pre>
<p>Now what happens when I find out that I would like to further restrict the range
of <code>id</code>, since there are too many of them from the previous result? One way
of doing it is to have compound Boolean expression inside the <code>numpy.where()</code>.</p>
<pre><code class="lang-python">I = numpy.where((table[&#39;id&#39;] &gt; 16000000) &amp; (table[&#39;id&#39;] &lt; 16005000))
xi = table[&#39;x&#39;][I]
</code></pre>
<p>That looks O.K. How about further restricting the result to entries with
positive <code>y</code> values?</p>
<pre><code class="lang-python">I = numpy.where((table[&#39;id&#39;] &gt; 16000000) &amp; (table[&#39;id&#39;] &lt; 16005000) &amp; (table[&#39;y&#39;] &gt; 0))
xi = table[&#39;x&#39;][I]
</code></pre>
<p>As you can see, the size and complexity of the compound Boolean expression will
quickly grow out of control. People will have hard time parsing and editing the
expression. What is worse, there is probably no way to truly <em>linearize</em> the
expression and thus decompose the query:</p>
<pre><code class="lang-python">I1 = numpy.where(table[&#39;id&#39;] &gt; 16000000)
I2 = numpy.where(table[&#39;id&#39;] &lt; 16005000)
xi = table[&#39;x&#39;][I1][][I2]
</code></pre>
<p>Since the table returned by <code>table[&#39;x&#39;][I1]</code> has different number of entries
than the original <code>table</code>, <code>I2</code> as calculated in the second line would
not be suitable for the third line. The only way of doing it would be:</p>
<pre><code class="lang-python">I1 = numpy.where(table[&#39;id&#39;] &gt; 16000000)
x1 = table[][I1]
I2 = numpy.where(x1[&#39;id&#39;] &lt; 16005000)
x2 = x1[&#39;x&#39;][I2]
</code></pre>
<p>Again, users are forced to create a whole lot of temporary variables. By the way,
I am also not so sure if <code>table[][I1][&#39;x&#39;][I2]</code> is syntactically correct for
the reason that subsetting/indexing a <em>table</em> may or may not return a <em>table</em> and
we may have to put a lot of parenthesis as well <code>(table[][I1])[&#39;x&#39;][I2]</code>.</p>
<p>The other problems is that array index notation also exposes the column major vs.
row major point of view of the underlying implementation. As you can see in his
examples, one of them uses row major point of view while the other uses column
major:</p>
<pre><code class="lang-python"># row major
xi = table[&#39;x&#39;][I]
# column major
xi = D.loc[D.id &gt; 16000000, [&#39;x&#39;]]
</code></pre>
<p>How and why programmers have to remember which one is which?</p>
<p>All of these problems are beautifully solved by something like R/dplyr or Haskell
which provides the bind/pipe operator. For example, I can easily linearize and
decompose my query into little pieces:</p>
<pre><code class="lang-R">table %&gt;% filter(id &gt; 16000000) %&gt;% filter(id &lt; 16005000) %&gt;% select(x)
</code></pre>
<p>The major reason this notation works is that at each stage of the <em>pipeline</em>,
<em>table</em> are used as both the input type and output type thus having <em>algebraic
closure</em>. We can in theory chain as many stages as we want without worry about if
we can not keep going because some operations are not supported thus falling of
the cliff all the sudden. There is also no need to worry about if the table is
actually row or column major. The concept is never in the play in the first
place.</p>
</p><a href="#Notation,-notation,-notation"><small>Link to this post</small></a><hr><br><h3>I don't like .func() chaining</h3><a name="I-don't-like-.func()-chaining" href="#I-don't-like-.func()-chaining" class="anchor"><span class="octicon octicon-link"></span></a><small>January 6, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>Or don&#39;t bind methods to data.</p>
<p>So, investigating some ways to introduce filter/select into various
languages, a colleague of mine and I both agree we don&#39;t like how 
Pandas and now Blaze is going to do it in Python. 
It looks too much like array syntax and filter and select are &quot;hidden&quot;.</p>
<pre><code class="lang-python">data[filterexpr, selectexpr][filterexpr, selectexpr]
</code></pre>
<p>Then, he brought up he prefers the R/dplyr syntax, which I agree with
(aside from the %&gt;% pipe operator, I prefer the F# one, |&gt;)</p>
<pre><code class="lang-R">data %&gt;% filter(expr) %&gt;% select(expr) %&gt;% filter(expr) %&gt;% select(expr)
</code></pre>
<p>And gave the example of how Scala does it and looks very similar
to pynq and asq for Python:</p>
<pre><code class="lang-python">data.filter(expr).select(expr).filter(expr).select(expr)
</code></pre>
<p>Though, I don&#39;t like that one. This assumes the methods are bound to
the data (i.e., object oriented programming). I really dislike it
in both Javascript and Python: the only time I use objects
in Python are when I do GUI programming because it lends itself well
to the object oriented design pattern.</p>
<p>functoolz has a pipe() function to compose, and I really like the |&gt;
operator in F#. I&#39;ve even implemented |&gt; in Haskell because I prefer
reading left to right, rather than right to left (the $ operator
or nested parens found in Scheme/Lisp).</p>
<p>In Python with functoolz:</p>
<pre><code class="lang-python">pipe(data, filter(expr), select(expr), filter(expr), select(expr))
</code></pre>
<p>In F#/Haskell with a |&gt; operator:</p>
<pre><code>data |&gt; filter(expr) |&gt; select(expr) |&gt; filter(expr) |&gt; select(expr)
</code></pre></p><a href="#I-don't-like-.func()-chaining"><small>Link to this post</small></a><hr><br><h3>FP in Python (II)</h3><a name="FP-in-Python-(II)" href="#FP-in-Python-(II)" class="anchor"><span class="octicon octicon-link"></span></a><small>November 19, 2014 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>After creating some data backed in a memmapped numpy array, I did a simple
filter and selection. There are over 16 million rows in this table.
The query selects <em>id</em>s greater than 16000000 and then uses the indices
to find <em>x</em>s with those <em>id</em>s.</p>
<p><em>numpy</em> does it in 0.03s, where <em>table</em> is a dict of numpy arrays:</p>
<pre><code class="lang-python">I = numpy.where(table[&#39;id&#39;] &gt; 16000000)
xi = table[&#39;x&#39;][I]
</code></pre>
<h3 id="updated-dec-16">Updated Dec 16</h3>
<p><em>D</em> is a <em>pandas</em> data frame created directly from the dictionary. 
<del>It&#39;s just as fast as <em>numpy</em>. Just make sure you use the 
right indexing, such as .loc, .iloc, and .ix. [] can be very slow.</del>
<em>Actually, doing some additional testing, I am finding that</em> pandas
<em>is an order of magnitude slower, what I was seeing before. I&#39;m not
sure what is going on here, but the operation below is .3s (I may
have read it incorrectly the first time.)</em></p>
<pre><code class="lang-python">xi = D.loc[D.id &gt; 16000000, [&#39;x&#39;]]
</code></pre>
<p><del>Though, we can see we are doing it out-of-core with the same memmapped 
<em>numpy</em> array with the nice <em>pandas</em> DSL. Yay!</del>
What I need to test next is
if I can get pandas to build out-of-core indices. <em>And, I need to verify
that I am still seeing that</em> pandas <em>is an order of magnitude slower
than numpy, which was my original conclusion prior to posting this.</em></p>
<p>And, here the loser is the iterator/generator method. sx and si are
&quot;streaming&quot; versions, created by numpy.nditer. PyObject overhead
rears its ugly head and this takes 47s... So, much for that.</p>
<pre><code class="lang-python">f = filter(lambda i: i[1] &gt; 16000000, zip(sx, si))
xi = [i[0] for i in f]
</code></pre>
</p><a href="#FP-in-Python-(II)"><small>Link to this post</small></a><hr><br><h3>FP in Python for Data Processing</h3><a name="FP-in-Python-for-Data-Processing" href="#FP-in-Python-for-Data-Processing" class="anchor"><span class="octicon octicon-link"></span></a><small>November 19, 2014 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>I recently came across several tools to be able to do functional
programming in Python. The ones in particular I am looking at are
<a href="https://github.com/kachayev/fn.py">fn</a> and 
<a href="http://toolz.readthedocs.org/en/latest/api.html">toolz</a>.
These extend a lot of the functionality that&#39;s already found in
<a href="https://docs.python.org/2/library/functools.html">functools</a> and 
<a href="https://docs.python.org/2/library/itertools.html">itertools</a>.
(Though, Guido has said Python was always intended to be an imperative
language and the functional stuff was never his personal agenda. This
is why <em>reduce</em> is deprecated because he thinks it is unreadable compared
to a <em>for</em> loop. I&#39;m paraphrasing here.)</p>
<p>So, why do this (aside from functional programming is a lot of 
fun in an academic way)? Because, it makes filtering and processing
of data pretty easy. In particular, joins, group by, filters (all
found in <em>SQL</em>) are very functional paradigms for data selection.</p>
<p>You ask, &quot;why not just use <em>pandas</em>?&quot; (<a href="http://pandas.pydata.org/">pandas</a>)
&quot;(or even <em>numpy</em> because you can do most of the <em>pandas</em> stuff in numpy?)&quot; 
&quot;It can do all of the filtering, joining, and data massaging that you&#39;d
like to do.&quot; Well, its because <em>pandas</em> is currently memory only, and frequently
our data sets are larger than the memory that you are currently running
on. (As an aside, you can back <em>numpy</em> arrays with memory maps, to do
out-of-core processing. It&#39;s really fast and transparent.) </p>
<p>Out-of-core (streaming) maps really well to a functional lazy
evaluation model, like Python generators. The aforementioned functional tools 
utilize on Python&#39;s lazy iterator/generator model and can be used
to process data out-of-core really easily. Plus, we have other work
going on that uses the vector/streaming/functional model for computation, found
in <a href="http://viz.lanl.gov/projects/PISTON.html">Piston</a>.</p>
<p>Though, I am keeping tabs on <a href="http://blaze.pydata.org">blaze</a>. It&#39;s not quite 
where I&#39;d like it, yet, (and being able to use it without Anaconda), 
but maybe it&#39;s the out-of-core <em>pandas</em> we&#39;d like to have, someday.
(In the same vein, someday I may be using <a href="http://julialang.org/">julia</a>, too.)</p>
</p><a href="#FP-in-Python-for-Data-Processing"><small>Link to this post</small></a><hr><br><h3>Python @ VIS 2014</h3><a name="Python-@-VIS-2014" href="#Python-@-VIS-2014" class="anchor"><span class="octicon octicon-link"></span></a><small>November 7, 2014 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>The tutorial for <a href="https://github.com/JosephCottam/PythonVIS2014">Python @ VIS 2014</a> is found <a href="https://github.com/JosephCottam/PythonVIS2014">here</a>.</p>
<iframe src="http://player.vimeo.com/video/103884477" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>

<p>See you in <a href="http://ieeevis.org/year/2014/info/overview-amp-topics/accepted-tutorials">Paris</a>!</p>
</p><a href="#Python-@-VIS-2014"><small>Link to this post</small></a><hr><br><h3>Updated Oct 23rd OSU Tutorial</h3><a name="Updated-Oct-23rd-OSU-Tutorial" href="#Updated-Oct-23rd-OSU-Tutorial" class="anchor"><span class="octicon octicon-link"></span></a><small>October 23, 2014 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>I&#39;ve updated the tutorial that I gave at OSU on beginner Python
and using numpy and matplotlib.</p>
<p>You can find the updated version on 
<a href="https://github.com/DataScienceAtScale/python-at-osu-oct-10">github</a>.</p>
<p>Thanks to 
<a href="http://web.cse.ohio-state.edu/~hwshen/hwshen/Welcome.html">Han-Wei Shen</a>, 
his <a href="http://web.cse.ohio-state.edu/~hwshen/hwshen/Research.html">students</a>, and
<a href="http://www.mcs.anl.gov/~tpeterka/">Tom Peterka</a> for attending, 
and helping me to prepare for the
<a href="http://ieeevis.org/">Visualization</a> tutorial coming up in a few weeks.</p>
</p><a href="#Updated-Oct-23rd-OSU-Tutorial"><small>Link to this post</small></a><hr><br><h3>Example SQL Queries (Part II)</h3><a name="Example-SQL-Queries-(Part-II)" href="#Example-SQL-Queries-(Part-II)" class="anchor"><span class="octicon octicon-link"></span></a><small>October 23, 2014 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>After starting to implement some of these 
<a href="#Example-SQL-Queries-on-HPC-Simulation-Data">queries</a>, I started
running into the problem that 
<a href="http://mpas-dev.github.io/ocean/ocean.html">MPAS</a> doesn&#39;t have a regular
grid. In particular, it has a Voronoi tesselated grid, which means
when you do integrations (or averages), this becomes a problem:
<code>select avg(u), avg(v), y, z group by x</code>.  </p>
<p>How do you group
by <em>x</em>, when every <em>x</em> is unique? In particular, the <em>xs</em> are floating point
data and don&#39;t line up exactly. <em>Group bys</em> are going to difficult, 
and we have seen this before in our experiments with 
<a href="http://link.springer.com/article/10.1007/s10586-014-0360-5">bitmap indices on floating point data</a>.</p>
<p>A first solution looks like we need to quantize the values we want to group by.
This is what we do in the bitmap indices, as floating point values
get quantized.
A better solution likely involes doing an actual integration with a kernel of
some stencil size with discrete <em>x</em> values, and searching for the cells
that sit under the kernel. </p>
<p>Though, that second method doesn&#39;t necessarily work well in a SQL
language. For now, lets just go with the first solution, because it
fits a SQL query better.</p>
</p><a href="#Example-SQL-Queries-(Part-II)"><small>Link to this post</small></a><hr><br><h3>Using Python for Visualization and Analysis Tutorials 2014</h3><a name="Using-Python-for-Visualization-and-Analysis-Tutorials-2014" href="#Using-Python-for-Visualization-and-Analysis-Tutorials-2014" class="anchor"><span class="octicon octicon-link"></span></a><small>October 3, 2014 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>I love Python. I&#39;ve basically done all of my research in Python since
2001 or so, and recently I&#39;m becoming a Python evangelist (though,
Python does have its drawbacks, like I wish it had pattern matching,
better support for immutable data so you could compile it (like
Haskell and F#), type inference for the same reason, better syntax 
for partial functions, etc... But I digress.)</p>
<p>Keeping with my recent evangelism,
I&#39;ll be at Ohio State, IEEE Vis 2014, and SIAM CSE 2015 this year teaching
Python for Visualization and Analysis tutorials with colleagues of mine at
Continuum Analytics, Kitware, and Indiana University.</p>
<ul>
<li>The Ohio State University, <em>October 10, 2014</em>, Columbus, OH</li>
<li><a href="http://ieeevis.org">IEEE VisWeek 2014</a>, <em>November 9-14, 2014</em>, Paris, France</li>
<li><a href="http://www.siam.org/meetings/cse15/">SIAM CSE 2015</a>, <em>March 14-18, 2014</em>, Salt Lake City, Utah</li>
</ul>
<p>Hope to see you there!</p>
</p><a href="#Using-Python-for-Visualization-and-Analysis-Tutorials-2014"><small>Link to this post</small></a><hr><br><h3>Example SQL Queries on HPC Simulation Data</h3><a name="Example-SQL-Queries-on-HPC-Simulation-Data" href="#Example-SQL-Queries-on-HPC-Simulation-Data" class="anchor"><span class="octicon octicon-link"></span></a><small>October 3, 2014 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>Recently, while working with some databases, 
I was trying to come up with some examples
of what a scientific workload might look like in a database.</p>
<p>This is because our typical workflow uses things other than
databases, like flat binary
files (hopefully column-major, though sometimes our scientists
do row-major), HDF5, NetCDF, VTK XML, etc. In rare circumstances,
they sometimes come in a text file. </p>
<p>Imagine the data management
nightmare going from: supercomputer, to parallel file system, to 
tape drives, and back again to do any analysis. It&#39;s slow, cumbersome
and a pain to manage a large-scale scientific data set in this way,
because I/O is always the bottleneck nowadays and it wastes
scientist time to move the data around from all the different
storage systems.  So a lot of my interest is trying to bring SQL 
(and the different implementations of it) as a DSL (domain specific language)
to subset, slice-and-dice, and query HPC data.</p>
<p>These are just some example queries I might imagine on
an MPAS-O ocean dataset, if they were stored in a SQL-like storage 
system, off the top of my head:</p>
<ul>
<li><code>select avg(temp), avg(salt) group by month</code></li>
<li><code>select avg(u), avg(v), y, z group by x</code></li>
<li><code>select avg(u) * avg(u) + avg(v) * avg(v), y, z group by x</code></li>
<li><code>select avg(temp), avg(salt) where month = 4</code></li>
<li><code>select avg(temp), avg(salt) where month = 4 and z = 0</code></li>
<li><code>select x, y, avg(u), avg(v), avg(salt), avg(temp) where month = 4 and z = 0 and x &gt; 1300 and x &lt; 1500 and y &gt; 1300 and y &lt; 1500 group by x, y</code></li>
<li><code>select x, y, salt where temp &gt; 20 and z = 0</code></li>
<li><code>select x, y, salt where temp &lt; 0 and z = 0</code></li>
<li><code>select x, y, salt where temp &lt; 0 and z &gt; 0</code></li>
<li><code>select avg(salt), avg(temp) group by week</code></li>
<li><code>select x, y, salt, temp where sqrt(u*u + v*v) &gt; 50 and z = 0</code></li>
</ul>
</p><a href="#Example-SQL-Queries-on-HPC-Simulation-Data"><small>Link to this post</small></a><hr><br><h3>Contrasting Currents: Highlighting Ocean Structures with Nested Colormaps</h3><a name="Contrasting-Currents:-Highlighting-Ocean-Structures-with-Nested-Colormaps" href="#Contrasting-Currents:-Highlighting-Ocean-Structures-with-Nested-Colormaps" class="anchor"><span class="octicon octicon-link"></span></a><small>October 2, 2014 -- <a href="https://github.com/cvcanada">Curt Canada</a><br></small><p><p>This is a visualization of ocean temperature, in degrees Celsius, from the MPAS-Ocean Model, developed at Los Alamos National Laboratory. The goal of the visualization is to enable detailed views of the eddies and currents in specific regions of the model. The color map developed allows for a detailed rendering of specific data ranges. The image of the North Atlantic shows that the Gulf Stream departs the US coast at Cape Hatteras, NC, and turns left just south of Greenland, a feature known as the &apos;Northwest Corner&apos;. This pathway compares well with observations, and is highlighted by a narrow range of greens on the colormap. In the western Pacific, the Kuroshio current is a narrow stream of warm water that sheds eddies as it departs eastward from Japan. The colormap was chosen to use light yellow at 22C at the center of the jet, highlighting cooler eddies to the north in blue and warmer eddies to the south in green. It was produced by Francesca G. Samsel of the Center for Agile Technologies at the University of Texas at Austin and Mark Petersen of the Climate, Ocean, Sea Ice Modeling Team at the Los Alamos National Laboratory.</p>
<p><img src="img/ContrastingCurrent.png" alt=""></p>
<p><a href="img/ContrastingCurrentLarge.jpeg">Larger images.</a></p>
</p><a href="#Contrasting-Currents:-Highlighting-Ocean-Structures-with-Nested-Colormaps"><small>Link to this post</small></a><hr><br></section></div><script src="javascripts/scale.fix.js"></script></body></html>